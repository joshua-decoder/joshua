lm_file=5gram.lm.gz

tm_file=dev.zh.grammar
tm_format=hiero

glue_file=hiero.glue
glue_format=hiero

#lm config
use_srilm=true
lm_ceiling_cost=100
use_left_equivalent_state=false
use_right_equivalent_state=false
order=3


#tm config
span_limit=10
phrase_owner=pt
mono_owner=mono
begin_mono_owner=begin_mono
default_non_terminal=X
goalSymbol=S

#pruning config
fuzz1=0.1
fuzz2=0.1
max_n_items=30
relative_threshold=10.0
max_n_rules=50
rule_relative_threshold=10.0

#nbest config
use_unique_nbest=true
use_tree_nbest=false
add_combined_cost=true
top_n=300


#remote lm server config, we should first prepare remote_symbol_tbl before starting any jobs
use_remote_lm_server=false
remote_symbol_tbl=./voc.remote.sym
num_remote_lm_servers=4
f_remote_server_list=./remote.lm.server.list
remote_lm_server_port=9000


#parallel deocoder: it cannot be used together with remote lm
num_parallel_decoders=1
parallel_files_prefix=/tmp/


###### model weights
#lm order weight
lm 1.0

#phrasemodel owner column(0-indexed) weight
phrasemodel pt 0 0.5
phrasemodel pt 1 0.5
phrasemodel pt 2 0.5

#arityphrasepenalty owner start_arity end_arity weight
#arityphrasepenalty pt 0 0 1.0
#arityphrasepenalty pt 1 1 -1.0
#arityphrasepenalty pt 2 2 -2.0

#arityphrasepenalty glue 1 1 1.0
#arityphrasepenalty glue 2 2 2.0

#phrasemodel mono 0 0.5

#wordpenalty weight
wordpenalty -1.0

#========= specify sparse feature def file here(this line should match the line in "params.txt.example") ==========
discriminative ./sparse_feat.example 1.0

#general
maxNumIter=10
useSemiringV2=true
maxNumHGInQueue=40
numThreads=20
printFirstN=10

#option for first feature (e.g., baseline feature)
normalizeByFirstFeature=true
fixFirstFeature=false

#loss-augmented pruning
lossAugmentedPrune=false
startLossScale=10
lossDecreaseConstant=2


#google linear corpus gain
useGoogleLinearCorpusGain=false
#googleBLEUWeights=-1.0;0.10277777076514476;0.07949965001350584;0.6993000659479868;0.09565585699195878
googleBLEUWeights=-1.0;0.2941176470588235;0.42016806722689076;0.6002400960384154;0.8574858514834507

#annealing?
#0: no annealing at all; 1: quenching only; 2: cooling and then quenching
annealingMode=0

isScalingFactorTunable=false
useL2Regula=false
varianceForL2=1
useModelDivergenceRegula=false
lambda=-1

#feature related
#dense features
useBaseline=false
baselineFeatureName=baseline_lzf
baselineFeatureWeight=1.0

useIndividualBaselines=true
baselineFeatIDsToTune=0;1

#sparse features
useSparseFeature=true

useRuleIDName=false

useTMFeat=false
useTMTargetFeat=false

useMicroTMFeat=true
wordMapFile=/media/Data/JHU/Research/MT_discriminative_LM_training/joshua_expbleu/TEST/mr/wordMap

useLMFeat=false
startNgramOrder=1
endNgramOrder=2
